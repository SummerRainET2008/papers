\section{Methodology}
In this section, we describe our proposed SFC method, which includes a basic 2-stage SFC implementation and a more compact version called joint SFC.

\subsection{2-stage SFC}

We first introduce this most intuitive implementation.
The outputs of the first stage are fed into the second stage, yet its parameters can not be tuned based on feedback of the second stage.

\subsubsection*{Stage 1: top-K candidate class labels generation} 
Any model that generates top-K most related class labels can be adopted in stage 1, such as classification models, term frequency-inverse document Frequency (TF-IDF) based retrieval models.
Due to the excellent performance of RoBERTa in practice, we use RoBERTa to construct a classification model in this stage. 

Suppose we have a training dataset $\mathcal{D}=\{x_{i},y_{i}\}_{i=1}^{N}$, in which $x_{i}$ is the user input query and $y_{i}$ is a single class label in $\{1, \ldots, J\}$. 
Given $x_i$, we take the final hidden state $h_i$ of the first token [CLS] encoded by RoBERTa as the representation for the whole sequence of $x_i$.
A linear layer is followed to output unnormalized probabilistic distribution of class labels, ${\Phi}_i^C=W^C h_i + b^C$, where $W^C$ and $b^C$ are class label related trainable parameters. 
Then, the loss in stage 1 classification model is shown in Equ. \ref{eq:classification_loss},

\vspace{-2em}
\begin{equation}
  \begin{aligned}
    \mathcal{L}^{C}&=\frac{1}{N} \sum_{i=1}^{N}-log(\frac{exp(\varPhi_{i,y_{i}}^C)}{\sum_{j=1}^{J}exp(\varPhi_{i,j}^C)}) \\
    \label{eq:classification_loss}
  \end{aligned}
\end{equation}
\vspace{-2.5em}

\subsubsection*{Stage 2: sentence-pair similarity model based multi-task learning}
We continue choosing RoBERTa to construct main modules in this stage.

We generate a sentence pair dataset $\mathcal{D'}=\{[(x,x')_{j},l_{j}]\}_{j=1}^{M}$ from $\mathcal{D}$, where $(x, x')_{j}$ is a pair of two history queries \footnote{For simplicity, we use query to denote a user's question or a response.}, and $l_{j}\in\{0, 1\}$ denotes the similarity that only the pair from the same class label is considered similar. 
The size of $\mathcal{D'}$ would be up to $N^2$ without a sampling strategy. 
This is prohibitively large and time-consuming for model training in stage 2, because $N$ can be several hundreds or thousands even under few shot setting in our scenario. 

Therefore, we adopt the idea of adversarial negative sampling strategy from the work \cite{bamler2020extreme}. 
The key idea is to train with negative samples that are hard to be distinguished from positive samples. 
Now suppose we have a user input query $x_{i}$ and top-$K$ most related class label set $\mathcal{C'}=\{c'_{1}, \dots, c'_{K}\}$ from stage 1, where $K$ is a hyperparameter, we apply the adversarial negative sampling strategy into our sentence pair sampling in two steps:

The first step is \emph{positive sentence pair sampling}. 
In training, we make sure the ground truth class label $y_{i}$ of $x_{i}$ exists in the candidate class label set $\mathcal{C'}$. 
Otherwise, we have it substitute for the least relevant class label $c'_{K}$. 
Then, we randomly sample $P$ sentences $\{x'_{1}, \dots, x'_{P}\}$ with the same class label as $y_{i}$ from dataset $\mathcal{D}$ to form the set of sentence pairs with positive label $\mathcal{P'}_{i}=\{[(x_{i}, x'_{1}), 1], \dots, [(x_{i}, x'_{P}), 1]\}$, where $P$ is also a hyperparameter. 

The second step is \emph{negative sentence pair sampling}.
We also randomly sample $P$ sentences for each class in the negative candidate class label set $\mathcal{C'}\backslash \{y_{i}\}$, which is the more confusing to the expected class label compared to those out of top-$K$ candidate set.
The resulting sentence pairs are strong adversarial negative samples for sentence-pair similarity model, and they significantly enhance the training speed and performance. 
We denote them as $\mathcal{N'}_{i}=\{[(x_{i}, x'_{1}), 0], ..., [(x_{i}, x'_{P\cdot K}), 0]\}$

In this way, we generate the sentence pair dataset $\mathcal{D'}$ for stage 2, based on the top-K class label set $\mathcal{C'}$ from by stage 1, as $\mathcal{D'}=\{\mathcal{P'}_{i}\cup \mathcal{N'}_{i}\}_{i=1}^{N}$, and its size is $M=N\times K\times P$.

As analyzed before, one advantage of similarity based model is its potential to use out-of-domain data. We take this advantage and use extra Quora dataset \cite{iyer2017first} to enhance performance of both baseline and our SFCs.

\subsubsection*{Multi-task based training}
We have our stage 2 model train on two tasks in the framework of multi-task training. 

The first task is \emph{regular sentence-pair similarity task}.
For sentence pair semantic similarity task, given a data point $(x, x')_{i}$ with similarity label $l_{i}$ from data set $\mathcal{D'}$, Roberta takes the concatenation of $x, x'$ and outputs hidden state $h_{i}$ of the first token [CLS] as its representation.
A linear layer is followed as ${\Phi}^S_{i}=W^Sh_{i}+b^S$ to generate unnormalized probabilities, where $W^S$ and $b^S$ are trainable parameters. 

We note that the dataset $\mathcal{D'}$ is unbalanced, as negative samples are $K - 1$ times as many as positive samples.
Therefore, we add a weight $w^S \in \{K-1, 1\} $ to the loss of positive and negative samples respectively to eliminate the bias, shown in Equ. \ref{eq:similarity_loss}.
This weighting strategy is quite effective in practice. 

\vspace{-1.5em}
\begin{equation}
  \begin{aligned}
    \mathcal{L}^{S}&=\frac{1}{M}\sum_{i=1}^{M}-w^S_{l_i}\cdot log(\frac{exp(\varPhi_{i,l_{i}}^S)}{\sum_{j=0}^{1}exp(\varPhi_{i,j}^S)}) \\
    \label{eq:similarity_loss}
  \end{aligned}
\end{equation}
\vspace{-2em}

The second task is \emph{classification on top-K classes}. 
As minimizing similarity loss is not our final goal of the chatbot application, we bring back classification loss again. 
In this task, the network structure is based on sentence pair similarity modules. 
We then add a task-specific average pooling layer, shown in Fig. \ref{fig:framework} to accomplish the classification task based on sentence-pair model.

The size of ${\Phi}^S$ in task 1, namely unnormalized probabilistic distribution, is $2 \times M$, as one sentence pair correspond to two class labels.
Further, it equals $(N\cdot K\cdot P)\times 2$, where $N$ is the size of original training data $\mathcal{D}$, $K$ the number of candidate class labels from stage 1, and $P$ is the number of sentences randomly sampled from each candidate class label. 
Now, for the average pooling layer, we first reshape ${\Phi}^S$ into the size of $N\times K\times P\times 2$, and then split it into ${\Phi}^{K,pos}$ and ${\Phi}^{K,neg}$, and both of them will have the size of $N\times K\times P$. 
In this way, ${\Phi}^{K,pos}$ represents the level of similarity for each sentence pair, and ${\Phi}^{K,neg}$ represents the level of dissimilarity for each sentence pair. 
Then, we can do average pooling for each candidate class among the top-K classes as shown in Equ. \ref{eq:average pooling}.

\vspace{-2em}
\begin{align}
  {\xi}_{i,j}^{K,pos} = \sum_{p=1}^{P}{\varPhi}_{i,j,p}^{K,pos} \ \ \ \ \ \ \ 
  {\xi}_{i,j}^{K,neg} = \sum_{p=1}^{P}{\varPhi}_{i,j,p}^{K,neg}
  \label{eq:average pooling}
\end{align}

With the average pooling result ${\xi}^{K,pos}$ and ${\xi}^{K,neg}$, we also
generate a top-K class label $l^{K}_i\in [1,\dots,K]$ for each
${\xi}^{K,pos}_{i}$ and ${\xi}^{K,neg}_{i}$. The loss for top-K classification
task is shown in Equ. \ref{eq:top-k_classification_loss}. In Equ. \ref{eq: pos
top-k loss} and Equ. \ref{eq: neg top-k loss}, the first term
$\xi_{i,l^{K}_{i}}^{K,pos}$ and $\xi_{i,l^{K}_{i}}^{K,neg}$ encourage high
level of similarity and low level of dissimilarity for prediction of the
correct label $l^{K}_i$ within the top-K candidate classes.

\begin{table*}
  \begin{centering}
    \scalebox{0.475}{
      \begin{tabular}{|c|cccccc|cccccc|cccccc|c|c|}
        \hline 
        \multicolumn{1}{|c|}{\multirow{2}*{\textbf{Models}} }& \multicolumn{6}{c|}{\textbf{CLINC150}}& \multicolumn{6}{c|}{\textbf{BANKING77}}& \multicolumn{6}{c|}{\textbf{HWU64}}& \ \ \ \textbf{ITG} \ \ \ & \textbf{Amazon-670k}\tabularnewline
        \cline{2-21}
        \multicolumn{1}{|c|}{} & $\textbf{5}$ & $\textbf{10}$ & $\textbf{15}$
        & $\textbf{20}$ & $\textbf{30}$ & $\textbf{50}$ & $\textbf{5}$ &
        $\textbf{10}$ & $\textbf{15}$ & $\textbf{20}$ & $\textbf{30}$ &
        $\textbf{50}$ & $\textbf{5}$ & $\textbf{10}$ & $\textbf{15}$ &
        $\textbf{20}$ & $\textbf{30}$ & $\textbf{50}$ & \textbf{3-fold} &
        \textbf{3-fold}\tabularnewline
        \hline
        TextCNN & \multirow{2}*{$0.5318$} & \multirow{2}*{$0.6963$} & \multirow{2}*{$0.7609$} & \multirow{2}*{$0.8142$} & \multirow{2}*{$0.8526$} & \multirow{2}*{$0.8867$} & \multirow{2}*{$0.4408$} & \multirow{2}*{$0.6436$} & \multirow{2}*{$0.7366$} & \multirow{2}*{$0.7918$} & \multirow{2}*{$0.8228$} & \multirow{2}*{$0.8619$} & \multirow{2}*{$0.3112$} & \multirow{2}*{$0.4007$} & \multirow{2}*{$0.4823$} & \multirow{2}*{$0.5272$} & \multirow{2}*{$0.5782$} & \multirow{2}*{$0.6262$} & \multirow{2}*{$0.6624$} & \multirow{2}*{$0.4401$}\tabularnewline
        (classification) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        LEAM & \multirow{2}*{$0.7514$} & \multirow{2}*{$0.8203$} & \multirow{2}*{$0.8612$} & \multirow{2}*{$0.8802$} & \multirow{2}*{$0.9010$} & \multirow{2}*{$0.9180$} & \multirow{2}*{$0.5422$} & \multirow{2}*{$0.7812$} & \multirow{2}*{$0.8129$} & \multirow{2}*{$0.8280$} & \multirow{2}*{$0.8610$} & \multirow{2}*{$0.8727$} & \multirow{2}*{$0.4545$} & \multirow{2}*{$0.5554$} & \multirow{2}*{$0.5936$} & \multirow{2}*{$0.6599$} & \multirow{2}*{$0.6855$} & \multirow{2}*{$0.7046$} & \multirow{2}*{$0.7086$} & \multirow{2}*{$0.6091$}\tabularnewline
        (classification) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        \hline
        BERT-large & \multirow{2}*{$0.8080$} & \multirow{2}*{$0.8904$} & \multirow{2}*{$0.9265$} & \multirow{2}*{$0.9334$} & \multirow{2}*{$0.9497$} & \multirow{2}*{$0.9595$} & \multirow{2}*{$0.5780$} & \multirow{2}*{$0.8004$} & \multirow{2}*{$0.8518$} & \multirow{2}*{$0.8827$} & \multirow{2}*{$0.8858$} & \multirow{2}*{$0.8982$} & \multirow{2}*{$0.4711$} & \multirow{2}*{$0.5963$} & \multirow{2}*{$0.6342$} & \multirow{2}*{$0.7010$} & \multirow{2}*{$0.7117$} & \multirow{2}*{$0.7424$} & \multirow{2}*{$0.7485$} & \multirow{2}*{$0.6658$}\tabularnewline
        (classification) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        ALBERT-xxlarge & \multirow{2}*{$0.8497$} & \multirow{2}*{$0.9008$} & \multirow{2}*{$0.9296$} & \multirow{2}*{$0.9297$} & \multirow{2}*{$0.9466$} & \multirow{2}*{$0.9578$} & \multirow{2}*{$0.5549$} & \multirow{2}*{$0.7981$} & \multirow{2}*{$0.8231$} & \multirow{2}*{$0.8530$} & \multirow{2}*{$0.8571$} & \multirow{2}*{$0.9096$} & \multirow{2}*{$0.4879$} & \multirow{2}*{$0.6116$} & \multirow{2}*{$0.6135$} & \multirow{2}*{$0.6996$} & \multirow{2}*{$0.7094$} & \multirow{2}*{$0.7376$} & \multirow{2}*{$0.7253$} & \multirow{2}*{$0.6893$}\tabularnewline
        (classification) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        RoBERTa-base & \multirow{2}*{$0.8732$} & \multirow{2}*{$0.9254$} & \multirow{2}*{$0.9363$} & \multirow{2}*{$0.9482$} & \multirow{2}*{$0.9558$} & \multirow{2}*{$0.9637$} & \multirow{2}*{$0.7305$} & \multirow{2}*{$0.8654$} & \multirow{2}*{$0.8808$} & \multirow{2}*{$0.9080$} & \multirow{2}*{$0.9061$} & \multirow{2}*{$0.9293$} & \multirow{2}*{$0.5831$} & \multirow{2}*{$0.6790$} & \multirow{2}*{$0.7064$} & \multirow{2}*{$0.7100$} & \multirow{2}*{$0.7320$} & \multirow{2}*{$0.7472$} & \multirow{2}*{$0.7734$} & \multirow{2}*{$0.6708$}\tabularnewline
        (classification) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        RoBERTa-large & \multirow{2}*{$\textbf{0.8974}$} &
        \multirow{2}*{$\textbf{0.9372}$} & \multirow{2}*{$\textbf{0.9508}$} &
        \multirow{2}*{$\textbf{0.9584}$} & \multirow{2}*{$\textbf{0.9621}$} & 
        \multirow{2}*{$\textbf{0.9733}$} & \multirow{2}*{$\textbf{0.7690}$} & 
        \multirow{2}*{$\textbf{0.8728}$} & \multirow{2}*{$\textbf{0.8966}$} & 
        \multirow{2}*{$\textbf{0.9099}$} & \multirow{2}*{$\textbf{0.9227}$} & 
        \multirow{2}*{$\textbf{0.9313}$} & \multirow{2}*{$\textbf{0.6044}$} & 
        \multirow{2}*{$\textbf{0.7002}$} & \multirow{2}*{$\textbf{0.7129}$} & 
        \multirow{2}*{$\textbf{0.7436}$} & \multirow{2}*{$\textbf{0.7493}$} & 
        \multirow{2}*{$\textbf{0.7678}$} & \multirow{2}*{$\textbf{0.7990}$} & 
        \multirow{2}*{$\textbf{0.7156}$}\tabularnewline
        (classification) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        \hline
        RoBERTa-large & \multirow{2}*{$0.8266$} & \multirow{2}*{$0.8861$} & \multirow{2}*{$0.9023$} & \multirow{2}*{$0.9084$} & \multirow{2}*{$0.9090$} & \multirow{2}*{$0.9407$} & \multirow{2}*{$0.757$} & \multirow{2}*{$0.8476$} & \multirow{2}*{$0.8614$} & \multirow{2}*{$0.8743$} & \multirow{2}*{$0.8749$} & \multirow{2}*{$0.8980$} & \multirow{2}*{$0.5425$} & \multirow{2}*{$0.6164$} & \multirow{2}*{$0.6503$} & \multirow{2}*{$0.6729$} & \multirow{2}*{$0.6947$} & \multirow{2}*{$0.7231$} & \multirow{2}*{$0.7418$} & \multirow{2}*{$0.6362$}\tabularnewline
        (similarity) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        \hline
        2-stage SFC & \multirow{2}*{$0.8979$} & \multirow{2}*{$0.9457$} & \multirow{2}*{$0.9517$} & \multirow{2}*{$0.9591$} & \multirow{2}*{$0.9610$} & \multirow{2}*{$0.9664$} & \multirow{2}*{$0.7975$} & \multirow{2}*{$0.8818$} & \multirow{2}*{$0.8962$} & \multirow{2}*{$0.9109$} & \multirow{2}*{$0.9187$} & \multirow{2}*{$0.9198$} & \multirow{2}*{$0.6477$} & \multirow{2}*{$0.7055$} & \multirow{2}*{$0.7200$} & \multirow{2}*{$0.7232$} & \multirow{2}*{$0.7484$} & \multirow{2}*{$0.7653$} & \multirow{2}*{$0.7972$} & \multirow{2}*{$0.7189$}\tabularnewline
        (task1) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        2-stage SFC & \multirow{2}*{$0.9162$} & \multirow{2}*{$0.9424$} & \multirow{2}*{$0.9530$} & \multirow{2}*{$0.9617$} & \multirow{2}*{$0.9633$} & \multirow{2}*{$0.9690$} & \multirow{2}*{$0.7997$} & \multirow{2}*{$0.8823$} & \multirow{2}*{$0.8945$} & \multirow{2}*{$0.9123$} & \multirow{2}*{$0.9236$} & \multirow{2}*{$0.9317$} & \multirow{2}*{$0.6498$} & \multirow{2}*{$0.6980$} & \multirow{2}*{$0.7202$} & \multirow{2}*{$0.7358$} & \multirow{2}*{$0.7498$} & \multirow{2}*{$0.7657$} & \multirow{2}*{$0.8020$} & \multirow{2}*{$0.7311$}\tabularnewline
        (task2) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        2-stage SFC & \multirow{2}*{$0.9167$} & \multirow{2}*{$0.9456$} & \multirow{2}*{$0.9571$} & \multirow{2}*{$0.9638$} & \multirow{2}*{$0.9658$} & \multirow{2}*{$0.9753$} & \multirow{2}*{$0.8135$} & \multirow{2}*{$0.8854$} & \multirow{2}*{$0.8931$} & \multirow{2}*{$0.9192$} & \multirow{2}*{$0.9257$} & \multirow{2}*{$0.9339$} & \multirow{2}*{$0.6525$} & \multirow{2}*{$0.7092$} & \multirow{2}*{$0.7168$} & \multirow{2}*{$0.7476$} & \multirow{2}*{$0.7519$} & \multirow{2}*{$0.7696$} & \multirow{2}*{$\textbf{0.8124}$} & \multirow{2}*{$0.7364$}\tabularnewline
        (task1 + task2) & & & & & & & & & & & & & & & & & & & &\tabularnewline
        \hline
        \multirow{2}*{Joint SFC} & \multirow{2}*{$\textbf{0.9231}$} & \multirow{2}*{$\textbf{0.9560}$} & \multirow{2}*{$\textbf{0.9644}$} & \multirow{2}*{$\textbf{0.9669}$} & \multirow{2}*{$\textbf{0.9712}$} & \multirow{2}*{$\textbf{0.9821}$} & \multirow{2}*{$\textbf{0.8270}$} & \multirow{2}*{$\textbf{0.9069}$} & \multirow{2}*{$\textbf{0.9103}$} & \multirow{2}*{$\textbf{0.9209}$} & \multirow{2}*{$\textbf{0.9323}$} & \multirow{2}*{$\textbf{0.9463}$} & \multirow{2}*{$\textbf{0.6697}$} & \multirow{2}*{$\textbf{0.7211}$} & \multirow{2}*{$\textbf{0.7254}$} & \multirow{2}*{$\textbf{0.7497}$} & \multirow{2}*{$\textbf{0.7593}$} & \multirow{2}*{$\textbf{0.7772}$} & \multirow{2}*{$0.8114$} & \multirow{2}*{$\textbf{0.7445}$}\tabularnewline
        & & & & & & & & & & & & & & & & & & & &\tabularnewline
        \hline
      \end{tabular}
    }
    \par
  \end{centering}
  \caption{
    F1 scores on five task-specific datasets for text classification in chatbot
    under low resource. For ITG, we keep the full dataset. For Amazon-670k, we
    randomly sampled 250 classes with training sample numbers within 5-15
    samples per class. For CLINC150, BANKING77, HWU64, we set up various
    few-shot settings (5/10/15/20/30/50 samples per class) while keeping the
    test set to be fixed. The highest scores among all the baseline models and SFC variants for each data setting are both marked
    in bold.
  }
  \label{tbe:table2}
\end{table*}

\vspace{-1em}
\begin{align}
  \mathcal{L}^{K} = \frac{1}{N}\sum_{i=1}^{N}(\mathcal{L}^{K,pos}_{i} - \mathcal{L}^{K,neg}_{i})
  \label{eq:top-k_classification_loss}
\end{align}
where
\begin{equation}
  \begin{aligned}
    \mathcal{L}^{K,pos}_{i} &= -log(\frac{exp(\xi_{i,l^{K}_{i}}^{K,pos})}{\sum_{j=1}^{K}exp(\xi_{i,j}^{K,pos})})
    \label{eq: pos top-k loss}
  \end{aligned}
\end{equation}

\begin{equation}
  \begin{aligned}
    \mathcal{L}^{K,neg}_{i} &= -log(\frac{exp(\xi_{i,l^{K}_{i}}^{K,neg})}{\sum_{j=1}^{K}exp(\xi_{i,j}^{K,neg})}) \\
    \label{eq: neg top-k loss}
  \end{aligned}
\end{equation}
\vspace{-1.25em}

Finally, the overall loss function for multi-task learning in stage 2 is
shown in Equ. \ref{eq:two-stage_SFC_loss}. The training objective of stage 2 is
to minimize the weighted sum of task-specific losses. Here $\alpha_S$ and
$\alpha_K$ are weights of task 1 and task 2 respectively.

\begin{align}
  \mathcal{L} = \alpha_S \mathcal{L}^S + \alpha_K \mathcal{L}^K
  \label{eq:two-stage_SFC_loss}
\end{align}
\vspace{-2em}

\subsection{Joint SFC}
In 2-stage SFC, two stages are separate that there is no deep interaction with each other during training. 
In this case, the performance of stage 1 may limit the model potential in stage 2; 
meanwhile, stage 2 also cannot loop feedback back to stage 1 in training for improvement. 
Therefore, we propose to jointly train all stages, shown in Fig. \ref{fig:framework}.

In joint SFC, a classification model is placed in stage 1 to dynamically provide top-K candidate class labels for the sentence-pair similarity model in stage 2 through a top-K pooling layer.
In this way, classification model and similarity model can be merged into one single model for multi-task training with sentence pairs sampled from varying candidate class labels.

There are 3 tasks in total during the training of joint SFC, shown in Fig. \ref{fig:framework}. 
The overall loss is a linear combination of them, shown in Equ. \ref{eq:joint_SFC_loss}. 
Here, $\alpha_S$, $\alpha_K$ and $\alpha_C$ represent the weights for task 1 sentence-pair similarity in Equ. \ref{eq:similarity_loss}, task 2 top-K classification in Equ. \ref{eq:top-k_classification_loss}, and task 3 general single sentence classification respectively in Equ.  \ref{eq:classification_loss}.

\vspace{-1.5em}
\begin{align}
  \mathcal{L} = \alpha_S \mathcal{L}^S + \alpha_K \mathcal{L}^K + \alpha_C \mathcal{L}^C
  \label{eq:joint_SFC_loss}
\end{align}
\vspace{-1.5em}

Likewise, our SFC can be strengthened by adding more beneficial tasks, such as an extra NER task based on the output states of a query from a pretrained model in stage 1. 
We do not pay much attention on details of these straightforward extensions, but on the whole network structure and training.
