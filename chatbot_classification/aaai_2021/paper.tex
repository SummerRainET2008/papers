%% LyX 2.3.1-1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[latin9]{inputenc}
\usepackage{array}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% Template for GlobalSIP-2018 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\usepackage{spconf}


\def\x{{\mathbf x}}
\def\L{{\cal L}}

\makeatother

\begin{document}
\title{     
  An attention based deep neural network for \\
  automatic lexical stress detection   
}
\name{Tian Xia, Xianfeng Rui, Chien-Lin Huang, Iek Heng Chu, Shaojun Wang, Mei Han}   
\address{     
  \{SummerRainET2008, huntrui, chiccocl, iekhengchu0217, swang.usa, hanmei613\}@gmail.com   
}
\maketitle 
\begin{abstract}
Lexical stress detection is one of important tasks in self-directed
language learning application. We address this task by leveraging
two successful attention techniques in natural language processing,
\textbf{inner attention} and \textbf{self-attention}. First, combined
with LSTM to model time-series features, inner attention could extract
most important information and then convert length-varying input into
a fixed-length feature vector; Second, self-attention intrinsically
support as input words with \emph{different number of syllables} to
model contexture information. Besides, our model is straightforward
to expand to include hand-crafted features to improve performance,
and can be applied to similar tasks, such as pitch accent detector.
Experiments on LibriSpeech, TedLium and a third self-recored datasets
show the high performance of our proposed attention based neural network. 
\end{abstract}
\keywords{stress, detection, attention, neural network}

\section{Introduction}

\label{sec:intro}

Nowadays self-directed language learning has become more and more
popular, and computer-aided pronunciation training therefore has drawn
considerable attention. The research in this direction is focusing
on mispronunciation detection and diagnosis problems by using speech
recognition related technologies. These problems can be addressed
in segmental level and suprasegmental level. For example, segmental
level work involving phones and words \cite{li2015integrating,li2016mispronunciation},
suprasegmental level involving lexical stress \cite{li2011a,li2012perceptually,li2013lexical}
and pitch accent \cite{li2011a,zhao2013exploiting}, etc. This paper
targets at lexical stress detection problem, which is one of most
important factors for evaluating proficiency.

Lexical stress has a relation with the prominent syllables of a word.
As pointed in the work of \cite{li2018automatic}, in many cases the
position of syllable carries important information to disambiguate
word semantics. e.g., ``subject" vs. "sub'ject", "permit" vs.
"per'mit". Once we detect lexical stress for a word, and compare
with its typical lexical stress pattern from dictionary, we could
determine whether stress pronunciation is correct.

There are a line of related work. Such as the work in \cite{tepperman2005automatic}
uses Gaussian mixture models; Zhao et al. \cite{svm} adopted SVMs
to identify the vowels of L2 English speech carrying primary stress
or not. Other similar studies using SVMs to detect lexical stress
of English speech with Taiwanese accent were reported in Wang et al.
\cite{wang2009stress}, Chen and Wang \cite{chen2010automatic}, etc.
Ferrer et al. \cite{ferrer2015classification} introduced a system
for lexical stress detection using both prosodic (pitch, energy and
duration) and spectral MFCC features. Results showed that the MFCC
helped improve the experimental performance. The error rates of the
system for L1-English and L1-Japanese data were 11\% and 20\% respectively.

Another important work in \cite{li2018automatic} uses multi-distribution
deep neural networks (MD-DNN), which are constructed by stacking up
multiple Restricted Boltzmann Machines (RBMs). The work from Li et
al. work uses several hand-crafted syllable-based features with length
fixed. The main difference with our model is we do not hand-craft
features, but have the neural network learn itself. Moreover, our
model does not require the input to be of 5-syllable\footnote{They would fill short words to 5-syllable with special values, usually
0s.}, and prevent any approximation of input.

\begin{figure*}[t]
\begin{centering}
\includegraphics[scale=0.45]{picture\lyxdot 1} 
\par\end{centering}
\caption{Our neural network structure. For example, first, the word ``\textbf{projector}''
is partitioned into three syllables by linguistic rules, ``pro'',
``jec'' and ``tor''. Second, each syllable, represented as concatenation
of several time-series features in the frame level, e.g., MFCC, pitch
and energy, is encoded by the LSTM blocks, and then be converted into
a fixed-length feature vector by inner attention Third, all syllable-representing
feature vectors are interacting with each other via\textbf{ self-attention},
and finally be trained to fit their final labels. Note that, all LSTM
models are sharing the same parameters, and so are  all position-wise
Feed-Forward-Networks (FFN).}
\label{fig:framework}
\end{figure*}

\section{Attention based neural network}

\label{sec:att-nn}

\subsection{Time-series features}

Intuitively, pitch highly correlates with stress. Instead of extracting
mere one highest frequency, we extract several highest frequencies
in each frame, which empirically contribute to the performance moderately..

As stressed syllable usually exhibits higher energy than its neighboring
ones, thus energy is extracted in the frame level.

Besides, Mel Frequency Cepstral Coefficient (MFCC) features are found
to benefit stress detection ask in \cite{ferrer2015classification}.
Hence we adopt MFCC here, with Delta and Delta-Delta information included
as well. Empirically, we found large dimension of MFCC improves the
final performance. Thus we prefer large dimensions.

All the three kinds of features above are normalized in the word level
and in each feature dimension. Our normalization strategy has two
parts. First, lineally scale features into the range of minimum and
maximum, and second, subtract by the mean value. Adopting either part
separately hurts the performance.

\subsection{Syllable encoding module}

The logic level 2, 3, 4 in Fig. \ref{fig:framework} demonstrate the
internal structure of syllable encoding module. It consists of one
bidirectional LSTM, several blocks of uni-directional LSTM and residual
edge, and one inner attention layer \cite{inneratten}.

Based on the statistics of syllable duration, we limit the maximum
LSTM steps as 50, corresponding to 500 ms duration. This setting is
the same through logic level 2 and 3. In logic level 2, two frame-level
LSTMs runs from different directions and element-wisely sum together
to enrich both the left and right context for each frame state. This
is also the common doing when applying LSTM into a sequential data.

The neural network structure in logic level 3 is alike in that of
Google translation model \cite{wu2016googlemt}. It is made of multiple
identical blocks, each of which has a uni-directional LSTM and element-wisely
add its input into its output via a residual edge.

In the logic level 4, inner-attention part, which can be interpreted
as a special weighted-pooling strategy, the processing is a bit tricky.
Because the durations of syllables vary a lot, it is beneficial to
only weight those real frame information, and ignore those filled
frame information due to the maximum LSTMS steps or maximum frame
number\footnote{Based on statistics of syllable durations, we just set maximum frame
number as 50.}. See Equ. \ref{eq:weight}.

\begin{align}
\alpha_{i}= & \begin{cases}
\textrm{softmax}\{f(\mathbf{S_{i}},\mathbf{H})\} & i\in[0,\textrm{syllable\_length})\\
0 & i\in[\textrm{syllable\_length},50)
\end{cases}\label{eq:weight}\\
\mathbf{S}= & \sum\alpha_{i}\cdot\mathbf{S_{i}}
\end{align}
where $\mathbf{S_{i}}$ is the state vector of LSTM, corresponding
to each speech frame. $\mathbf{H}$ is a global and trainable vector,
shared by each syllable.The function $f$ defines how to compute the
importance of each state vector by its real content. The simplest
definition is the inner product. 

\subsection{Syllable interaction module}

Pronunciation stress kinds of reflects the focused information of
a whole word, thus, it is natural to consider contextual information.
The work in \cite{li2018automatic} also take advantage of context
of a syllable, but they considers fixed four-syllable contextual information.
In comparison, we take a self-attention \cite{bert,transformer} based
network, which digests words with different number of syllables as
input. There is no need to expand input by filling empty positions.

The logic level 5 includes two parts, $\mathcal{O}(n^{2})$ operations
of self-attention, and $\mathcal{O}(n)$ operations of position-wise
feed forward network. 

In the self-attention part, we adopt more efficient bi-linear formula
than inner product for the attention weight $\alpha_{i,j}$, and the
matrix $\mathbf{M}$ is a globally trainable parameter. There are
other alternatives to calculate the weight $\alpha_{i,j}$, such as
more powerful multi-head attention in BERT model \cite{bert,transformer}.
It is not difficult to replace with it, however, we adopt the bi-linear
formula due to two reasons, simple to implement and focused on the
whole network structure itself.

\begin{align}
\alpha_{i,j}= & \textrm{softmax}\{\mathbf{S_{i}}^{T}\mathbf{M}\mathbf{S_{j}}\}\label{eq:self-att-weight}\\
\mathbf{S_{i}}= & \sum_{j}\alpha_{i,j}\cdot\mathbf{S_{j}}
\end{align}

In the position-wise feed forward network part, we use the definition
in \cite{transformer}. It is actually equivalent to two dense networks,
with the first one having the relu activation function and the second
one not.

\begin{equation}
\textrm{FFN}(x)=\textrm{max}(0,xW_{1}+b_{1})W_{2}+b\label{eq:ffn}
\end{equation}


\subsection{Target label}

Generally, we assign scores of 1, 0.5, 0 \footnote{It is only based on experiences, and might not the optimal.}
as our target labels for primary stress, secondary stress, and no
stress respectively.We then use $l1$-norm to convert these label
scores into a probability distribution, and use in our cross-entropy
based loss function. We should note that one word might have more
than one primary stress, thus problem is a not multi-class problem,
but a multi-label problem. 

\begin{equation}
\mathcal{L}=-\sum_{syl}p_{\textrm{syl}}^{\textrm{label}}\log P_{syl}^{\textrm{o}}
\end{equation}
where $p_{\textrm{syl}}^{\textrm{label}}$ is the normalized target
label probability of some syllable, and $P_{syl}^{\textrm{o}}$ is
the corresponding output probability from the self-attention blocks.

\section{Experiments}

\subsection{Datasets}

As there lack public datasets with enough wrong stress pronunciations,
we adopt two public datasets, which is usually used for ASR task,
to evaluate the performance of detected\textbf{ true stress }pronunciations;
we also recored a third dataset with one half of wrong stress pronunciation,
to evaluate the performance of detected \textbf{wrong stress} pronunciations. 

In practical application, users are more concerned about the correctness
of the primary stress, so we do not predict and measure the \textbf{secondary
stress}, though our model is capable. 

Besides, our model is end-to-end based to directly optimize the stress
prediction in the \textbf{word-level}, thus, we do not report measures
of syllable-level stress prediction. All reported results in the experiments
are F-values, which balances the precision rate and recall rate.
\begin{enumerate}
\item LibriSpeech \footnote{http://www.openslr.org/12} dataset, and we
use 360 hours clean read English speech for training, 50 hours for
testing.
\item TedLium dataset, which is talk set with a variety of speakers and
topics. We use 400 hours for training, 50 hours for testing.
\item dictionary based dataset. We have 10 teammates record 2000 vocabularies,
most of which have 3 syllable and 4 syllables \footnote{2-syllable words are much easier. See Table \ref{tbe:total}.},
and pronuciate each word for three time. Totally we have about 6000
word based samples, and a half is from female speakers.
\end{enumerate}
\begin{table*}
\begin{centering}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
Dataset & Model & \emph{\#syllable}=all & 2 & 3 & 4 & 5\tabularnewline
\hline 
\multirow{3}{*}{LibriSpeech} & SVM & 0.80 & 0.822 & 0.783 & 0.730 & 0.652\tabularnewline
\cline{2-7} 
 & Boosting & 0.85 & 0.874 & 0.834 & 0.784 & 0.708\tabularnewline
\cline{2-7} 
 & Ours(summer) & \textbf{0.95} & 0.9619 & 0.9138 & 0.8304 & 0.7232\tabularnewline
\hline 
\multirow{3}{*}{TedLium} & SVM & 0.812 & 0.834 & 0.779 & 0.739 & 0.661\tabularnewline
\cline{2-7} 
 & Boosting & 0.862 & 0.888 & 0.831 & 0.813 & 0.732\tabularnewline
\cline{2-7} 
 & Ours & \textbf{0.951} & 0.9669 & 0.9438 & 0.8804 & 0.7832\tabularnewline
\hline 
\multirow{3}{*}{dictionary} & SVM & 0.69 & 0.712 & 0.682 & 0.682 & 0.643\tabularnewline
\cline{2-7} 
 & Boosting & 0.726 & 0.734 & 0.721 & 0.712 & 0.654\tabularnewline
\cline{2-7} 
 & Ours & \textbf{0.788} & 0.821 & 0.777 & 0.762 & 0.723\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{In LibriSpeech and TedLium datasets have its own training and testing
data, while as dictionary-data is limited, we just take the best model
trained in LibriSpeech data. Our model adopts the best configuration.
More details can be found in Table \ref{tbe:lstm-level} and \ref{tbe:self-att-level}.}

\label{tbe:total}
\end{table*}

\subsection{Features and systems}

All features used in our systems and baseline systems are based on
syllable. Here for our syllable feature extraction, we use the Maximal
Onset Principle (Pulgram, 1970) to extract syllables from the phoneme
sequence decoded from our automatic speech recognizer. 

Four kinds of features are considered in baseline systems, and they
are duration, energy, pitches, and MFCC. Since the absolute duration
of the same syllable within one word can vary from people to people,
we actually measures the relative duration of each syllable within
one word. Regarding the other features, we process in the same way.
They are extracted in the frame level, and normalized at the word
boundary to compute its relative values. We chose 25\% percentile,50\%
percentile, 75\% percentile, minimum and maximum value within the
syllable window. Regarding MFCC, we observe higher dimension benefits
better results. Therefore, we set the dimension of MFCC to 40. Plus
additional delta and delta-delta information, we actually use 120
dimension MFCC related features.
\begin{enumerate}
\item The first baseline system is a SVM based system which is similar to
the one in \cite{svm}. We use Gaussian kernel and default parameter
settings.
\item The second baseline baseline system is gradient-boosting tree based,
sharing exact features with the first baseline. There are some tunable
parameters, such as tree depth, leave number. We report the best result. 
\item Our attention based network model is capable of direct processing
time series features, thus we take energy, pitches and MFCC in all
frames, but we do not use the duration feature. The model is implemented
in Tensorflow, and the optimizer is Adam with default hyper parameters.
The learning rate is 1e-3. We found 10 epoch of train is enough to
reach good performance.
\end{enumerate}

\subsection{Results}

We first look at the performances of all models on the whole datasets
in Table \ref{tbe:total}. Boosting tree based training performs consistently
better than SVM based, because boosting tree has more controllable
parameters to tune in practice. More, tree-based models have no requirement
in the data normalization, and it is hence more robust. Our attention
based network model performs significantly better than boosting tree
based. This is our model theoretically digests time-series features,
and they contains more information than the input feeding into SVM
and boosting tree based . 

In LibriSpeech and TedLium datasets, our model has much better ability
to sufficiently fit the data. However, this advantages moderately
decrease in the self-recorded dictionary dataset. We consider there
are two reasons. The first one is different data distribution. The
dictionary dataset is from Chinese accent speakers, while LibriSpeech
data is American English, and TedLium is mixed with a variety of speakers
with different accent. The second one is, both LibriSpeech and TedLium
datasets are supposed to not include wrong stress pronunciations.
This might result in sort of memorizing vocabularies, as well as their
stresses, with any feature set. We could not exclude this possibility,
as it is hard to create a dataset with enough wrong stresses to verify
it. Anyway, our model still reveals better performance than baselines
comparatively. To construct a large dataset with wrong stresses to
improve our model is also our future work.

As the distribution of syllable number of words varies in datasets,
we further have a statistics of model performances on words with different
syllable number. In Table \ref{tbe:total}, generally, our model on
short words, which has no more than 3 syllables, performs quite well
with at least 0.90 F-values. On longer words, all models perform worse,
and our model still holds advance over other two baselines.

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
\emph{\#LSTM} & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
\hline 
\multirow{1}{*}{LibriSpeech} & 0.92 & 0.928 & 0.939 & 0.944 & \textbf{0.951} & 0.948\tabularnewline
\hline 
\multirow{1}{*}{dictionary} & 0.743 & 0.751 & 0.760 & 0.768 & \textbf{0.77} & 0.764\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{We show the performances from different block numbers of LSTM in the
logic layer 3 in Figure \ref{fig:framework}, where the self-attention
block number is fixed to 1. See Table \ref{tbe:self-att-level}.}

\label{tbe:lstm-level}
\end{table}
In Table \ref{tbe:lstm-level}, we display the performances from different
number of LSTM blocks. Besides testing on the LibriSpeech data, we
also apply the best model into dictionary dataset for testing. We
observed that more LSTM layers strongly contributes to the final performance.
However, when the layer number reaches 6, the performance decreases.
Besides, more LSTM blocks make the training significantly slower. 

\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|c|}
\hline 
\emph{\#self-att} & 0 & 1 & 2\tabularnewline
\hline 
\multirow{1}{*}{LibriSpeech} & 0.941 & \textbf{0.951} & 0.929\tabularnewline
\hline 
\multirow{1}{*}{dictionary} & 0.743 & \textbf{0.77} & 0.760\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{We show the performances from different number of self-attention blocks
in the logic layer 5 in Figure \ref{fig:framework}, where the LSTM
block number is fixed to 5.}

\label{tbe:self-att-level}
\end{table}
In Table \ref{tbe:self-att-level}, we surprisingly find more self-attention
layers are not helpful to this task, which kind of differs from that
in machine translation experiments \cite{transformer}. We conjecture
our task is comparatively simpler than machine translation task, and
more self-attention layers lead to overfitting. We also tried other
position-wise activation function in Equ. \ref{eq:ffn}, such as one
layer linear network with relu or other activations, and they are
not helpful. 

\section{Conclusion}

In this work, we successfully applied two widely used attention techniques
into lexical stress detection problem. Our proposed model directly
takes time series features as input that it could fully explore input
information; and the network structure instrinstically support words
with different number of syllables, without expanding short words,
to reduce input approximation. Experiments also demonstrate the improvements
over baselines systems.

\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
